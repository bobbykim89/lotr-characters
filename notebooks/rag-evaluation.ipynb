{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from tqdm import tqdm\n",
    "from os import environ\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "from composables.files import open_json_file, save_json_file\n",
    "from composables.search import llm, format_hits_response\n",
    "from composables.data_processing import format_list_in_batch\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfead1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = environ.get('QDRANT_URL')\n",
    "QDRANT_API_KEY = environ.get('QDRANT_API_KEY')\n",
    "COLLECTION_NAME = 'lotr-characters'\n",
    "EMBEDDING_DIMENSION = 512\n",
    "JINA_EMBEDDING_MODEL = \"jina-embeddings-v4\"\n",
    "JINA_URL = \"https://api.jina.ai/v1/embeddings\"\n",
    "JINA_API_KEY = environ.get('JINA_API_KEY')\n",
    "QUERYING_TASK = \"retrieval.query\"\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "OPENAI_TEMPERATURE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ec3f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "qd_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4f6f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rag_prompt (query: str, search_results: list[dict[str,str]]):\n",
    "    raw_user_prompt = \"\"\"\n",
    "Context from database:\n",
    "{retrieved_context}\n",
    "\n",
    "User question:\n",
    "{user_question}\n",
    "\n",
    "Answer the question using ONLY the context above.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "You are a helpful lore expert on J.R.R. Tolkien's Middle-earth. \n",
    "You can only answer questions about characters using the provided context retrieved from the database. \n",
    "The context includes structured information such as: name, race, titles, realm, family relations, birth and death dates, and short descriptions.\n",
    "\n",
    "Guidelines:\n",
    "- If the answer is found in the context, respond clearly and directly.\n",
    "- If the answer is not in the context, say you donâ€™t know or that the information was not provided.\n",
    "- Do not invent new facts outside the context.\n",
    "- Keep your answers concise, but include all relevant details from the context.\n",
    "- If the user asks for speculation (e.g., \"what would happen if X met Y?\"), you can summarize based only on what the context says about their traits.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    user_prompt = raw_user_prompt.format(retrieved_context=search_results, user_question=query).strip()\n",
    "    return user_prompt, system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df0b043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_eval_prompt (payload: dict[str,str])-> tuple[str, str]:\n",
    "    raw_user_prompt = \"\"\"\n",
    "Evaluate the following RAG output.\n",
    "{{\n",
    "  \"question\": \"{question}\",\n",
    "  \"context\": \"{context}\",\n",
    "  \"answer\": \"{answer}\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "You are an impartial evaluator assessing the quality of a RAG (Retrieval-Augmented Generation) system that answers questions about J.R.R. Tolkienâ€™s Middle-earth characters.\n",
    "\n",
    "You will receive a JSON input with the following fields:\n",
    "{\n",
    "  \"question\": \"<user query>\",\n",
    "  \"context\": \"<retrieved context>\",\n",
    "  \"answer\": \"<model-generated answer>\"\n",
    "}\n",
    "\n",
    "Your task is to evaluate how well the answer satisfies the question, using only the information in the context.\n",
    "\n",
    "Evaluate on four criteria:\n",
    "1. Relevance â€” Does the answer directly address the question?\n",
    "2. Groundedness â€” Are all facts supported by the provided context (no hallucinations)?\n",
    "3. Completeness â€” Does the answer include all key details from the context?\n",
    "4. Faithfulness â€” Does it follow the system rules (concise, factual, no invention, admits missing info)?\n",
    "\n",
    "Scoring Guide (0â€“3 for each):\n",
    "- 3: Excellent â€” fully meets the criterion\n",
    "- 2: Fair â€” mostly correct, minor omissions or minor unsupported detail\n",
    "- 1: Weak â€” noticeable errors, missing or irrelevant info\n",
    "- 0: None â€” fails completely or contradicts context\n",
    "\n",
    "Your output must be a single valid JSON object:\n",
    "{\n",
    "  \"relevance\": <0â€“3>,\n",
    "  \"groundedness\": <0â€“3>,\n",
    "  \"completeness\": <0â€“3>,\n",
    "  \"faithfulness\": <0â€“3>,\n",
    "  \"comments\": \"<1â€“2 sentence summary of reasoning>\"\n",
    "}\n",
    "\n",
    "Output only the JSON object â€” no markdown, no extra text.\n",
    "\"\"\".strip()\n",
    "    user_prompt = raw_user_prompt.format(question=payload.get('question'), context=payload.get('context'), answer=payload.get('answer')).strip()\n",
    "    \n",
    "    return user_prompt, system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2493af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_search_result (data: dict, query: str):\n",
    "    search_result = data.get('search_results')\n",
    "    formatted_search_result = format_hits_response(hits=search_result)\n",
    "    user_prompt, system_prompt = format_rag_prompt(query=query, search_results=formatted_search_result)\n",
    "    res = llm(user_prompt=user_prompt, system_prompt=system_prompt)\n",
    "    return res, formatted_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a4ab9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_eval_llm_as_judge (query: str):\n",
    "    answer, search_result = rag_with_search_result(query=query)\n",
    "    payload = {\n",
    "        \"question\": query,\n",
    "        \"context\": search_result,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    user_prompt, system_prompt = format_eval_prompt(payload=payload)\n",
    "    res = llm(user_prompt=user_prompt, system_prompt=system_prompt)\n",
    "    \n",
    "    if type(res) == str:\n",
    "        json_res = json.loads(res)\n",
    "        return {\"question\": query, \"answer\": answer, **json_res}\n",
    "    else:\n",
    "        return {\"question\": query, \"answer\": answer, **res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a0964623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_eval_with_retrieval_results(data: dict):\n",
    "    search_result = data.get('search_results')\n",
    "    question = data.get('question')\n",
    "    formatted_search_result = format_hits_response(hits=search_result)\n",
    "    rag_user_prompt, rag_sys_prompt = format_rag_prompt(query=question, search_results=formatted_search_result)\n",
    "    answer = llm(user_prompt=rag_user_prompt, system_prompt=rag_sys_prompt)\n",
    "    payload = {\n",
    "        \"question\": question,\n",
    "        \"context\": search_result,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    eval_user_prompt, eval_sys_prompt = format_eval_prompt(payload=payload)\n",
    "    res = llm(user_prompt=eval_user_prompt, system_prompt=eval_sys_prompt)\n",
    "\n",
    "    if type(res) == str:\n",
    "        json_res = json.loads(res)\n",
    "        return {\"question\": question, \"answer\": answer, **json_res}\n",
    "    else:\n",
    "        return {\"question\": question, \"answer\": answer, **res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e4b99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_eval_result_with_retrieval_resutls(data: list[dict]):\n",
    "    eval_results = []\n",
    "    for retrieval_result in tqdm(data, desc=\"Processing documents\"):\n",
    "        result = rag_eval_with_retrieval_results(data=retrieval_result)\n",
    "        eval_results.append(result)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "620c8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_result(data: list=None, previous_results=None, start_index=0, requests_per_minute=400):\n",
    "    rag_results = previous_results if previous_results is not None else []\n",
    "    current_index = start_index\n",
    "\n",
    "    # Calculate delay between requests to stay under rate limit\n",
    "    delay_seconds = 60.0 / requests_per_minute\n",
    "\n",
    "    try:\n",
    "        for obj in tqdm(data, desc=\"Processing documents\"):\n",
    "            doc_id = obj[\"id\"]\n",
    "            for q_idx, question in enumerate(obj[\"questions\"]):\n",
    "                if current_index < start_index:\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    answer, search_result = rag_with_search_result(query=question)\n",
    "                    if answer is None:\n",
    "                        raise ValueError(\"Search returned None\")\n",
    "                    result = {\n",
    "                        \"question\": question,\n",
    "                        \"answer\": answer,\n",
    "                        \"search_result\": search_result\n",
    "                    }\n",
    "                    rag_results.append(result)\n",
    "                    current_index += 1\n",
    "\n",
    "                    # Add delay to respect rate limit\n",
    "                    time.sleep(delay_seconds)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nâŒ Error at index {current_index}\")\n",
    "                    print(f\"   Document ID: {doc_id}\")\n",
    "                    print(f\"   Question {q_idx + 1}/{len(obj['questions'])}: {question}\")\n",
    "                    print(f\"   Error: {type(e).__name__}: {str(e)}\")\n",
    "                    print(f\"\\nðŸ’¾ Processed {len(rag_results)} questions before failure\")\n",
    "                    print(f\"   Returning (relevance_total, {current_index}) for resume\")\n",
    "                    return rag_results, current_index\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\nâš ï¸  Interrupted by user at index {current_index}\")\n",
    "        print(f\"ðŸ’¾ Processed {len(rag_results)} questions\")\n",
    "        return rag_results, current_index\n",
    "    \n",
    "    return rag_results, current_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ee59738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_result(data: list=None, previous_results=None, start_index=0, requests_per_minute=400):\n",
    "    eval_results = previous_results if previous_results is not None else []\n",
    "    current_index = start_index\n",
    "\n",
    "    # Calculate delay between requests to stay under rate limit\n",
    "    delay_seconds = 60.0 / requests_per_minute\n",
    "\n",
    "    try:\n",
    "        for obj in tqdm(data, desc=\"Processing documents\"):\n",
    "            doc_id = obj[\"id\"]\n",
    "            for q_idx, question in enumerate(obj[\"questions\"]):\n",
    "                if current_index < start_index:\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    result = rag_eval_llm_as_judge(query=question)\n",
    "                    if result is None:\n",
    "                        raise ValueError(\"Search returned None\")\n",
    "                    \n",
    "                    eval_results.append(result)\n",
    "                    current_index += 1\n",
    "\n",
    "                    # Add delay to respect rate limit\n",
    "                    time.sleep(delay_seconds)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nâŒ Error at index {current_index}\")\n",
    "                    print(f\"   Document ID: {doc_id}\")\n",
    "                    print(f\"   Question {q_idx + 1}/{len(obj['questions'])}: {question}\")\n",
    "                    print(f\"   Error: {type(e).__name__}: {str(e)}\")\n",
    "                    print(f\"\\nðŸ’¾ Processed {len(eval_results)} questions before failure\")\n",
    "                    print(f\"   Returning (relevance_total, {current_index}) for resume\")\n",
    "                    return eval_results, current_index\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\nâš ï¸  Interrupted by user at index {current_index}\")\n",
    "        print(f\"ðŸ’¾ Processed {len(eval_results)} questions\")\n",
    "        return eval_results, current_index\n",
    "    \n",
    "    return eval_results, current_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d6529d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_evaluation_result (file_path: str):\n",
    "    eval_data: list[dict] = open_json_file(file_path=file_path)\n",
    "    num_entries = len(eval_data)\n",
    "    \n",
    "    relevance_total = 0\n",
    "    groundedness_total = 0\n",
    "    completeness_total = 0\n",
    "    faithfulness_total = 0\n",
    "\n",
    "    for entry in tqdm(eval_data, desc=\"Processing data\"):\n",
    "        relevance = entry.get(\"relevance\")\n",
    "        groundedness = entry.get(\"groundedness\")\n",
    "        completeness = entry.get(\"completeness\")\n",
    "        faithfulness = entry.get(\"faithfulness\")\n",
    "\n",
    "        relevance_total += relevance\n",
    "        groundedness_total += groundedness\n",
    "        completeness_total += completeness\n",
    "        faithfulness_total += faithfulness\n",
    "    \n",
    "    avg_relevance = relevance_total / num_entries\n",
    "    avg_groundedness = groundedness_total / num_entries\n",
    "    avg_completeness = completeness_total / num_entries\n",
    "    avg_faithfulness = faithfulness_total / num_entries\n",
    "    total_avg_score = (relevance_total + groundedness_total + completeness_total + faithfulness_total) / (num_entries * 4)\n",
    "\n",
    "    print(\"\"\"\n",
    "Evaluate on four criteria:\n",
    "1. Relevance â€” Does the answer directly address the question?\n",
    "2. Groundedness â€” Are all facts supported by the provided context (no hallucinations)?\n",
    "3. Completeness â€” Does the answer include all key details from the context?\n",
    "4. Faithfulness â€” Does it follow the system rules (concise, factual, no invention, admits missing info)?\n",
    "\n",
    "Scoring Guide (0â€“3 for each):\n",
    "- 3: Excellent â€” fully meets the criterion\n",
    "- 2: Fair â€” mostly correct, minor omissions or minor unsupported detail\n",
    "- 1: Weak â€” noticeable errors, missing or irrelevant info\n",
    "- 0: None â€” fails completely or contradicts context\n",
    "\"\"\")\n",
    "    print(f\"Number of entries: {num_entries}\")\n",
    "    print(f\"Average Relevance Score: {avg_relevance}\")\n",
    "    print(f\"Average Groundedness Score: {avg_groundedness}\")\n",
    "    print(f\"Average Completeness Score: {avg_completeness}\")\n",
    "    print(f\"Average Faithfulness Score: {avg_faithfulness}\")\n",
    "    print(f\"Total Average Score: {total_avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e2cfe7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 749\n"
     ]
    }
   ],
   "source": [
    "golden_questions = open_json_file(file_path=\"../dist/golden_questions.json\")\n",
    "raw_search_results = open_json_file(file_path=\"../dist/retrieval_search_results.json\")\n",
    "batched_data = format_list_in_batch(data=golden_questions, batch_size=50)\n",
    "golden_questions_batch_1 = batched_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7ae4a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [34:20<00:00,  4.12s/it]  \n"
     ]
    }
   ],
   "source": [
    "eval_results = generate_rag_eval_result_with_retrieval_resutls(data=raw_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e84cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_file(data=eval_results, file_path=\"../dist/evaluation_results_gpt_4o_mini.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cc97a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 2413293.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on four criteria:\n",
      "1. Relevance â€” Does the answer directly address the question?\n",
      "2. Groundedness â€” Are all facts supported by the provided context (no hallucinations)?\n",
      "3. Completeness â€” Does the answer include all key details from the context?\n",
      "4. Faithfulness â€” Does it follow the system rules (concise, factual, no invention, admits missing info)?\n",
      "\n",
      "Scoring Guide (0â€“3 for each):\n",
      "- 3: Excellent â€” fully meets the criterion\n",
      "- 2: Fair â€” mostly correct, minor omissions or minor unsupported detail\n",
      "- 1: Weak â€” noticeable errors, missing or irrelevant info\n",
      "- 0: None â€” fails completely or contradicts context\n",
      "\n",
      "Number of entries: 500\n",
      "Average Relevance Score: 2.998\n",
      "Average Groundedness Score: 2.992\n",
      "Average Completeness Score: 2.738\n",
      "Average Faithfulness Score: 2.99\n",
      "Total Average Score: 2.9295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_evaluation_result(file_path=\"../dist/evaluation_results_gpt_4o_mini.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7035b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_eval_df = pd.DataFrame(data=eval_results)\n",
    "gpt_4o_mini_eval_df.to_csv(\"../dist/evaluation_results_gpt_4o_mini.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
