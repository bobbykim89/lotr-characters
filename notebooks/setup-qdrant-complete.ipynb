{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "996d29a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import json\n",
    "import requests\n",
    "from os import environ\n",
    "import tiktoken\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = environ.get('QDRANT_URL')\n",
    "QDRANT_API_KEY = environ.get('QDRANT_API_KEY')\n",
    "COLLECTION_NAME = 'lotr-characters'\n",
    "EMBEDDING_DIMENSION = 512\n",
    "JINA_EMBEDDING_MODEL = \"jina-embeddings-v4\"\n",
    "JINA_URL = \"https://api.jina.ai/v1/embeddings\"\n",
    "JINA_API_KEY = environ.get('JINA_API_KEY')\n",
    "INDEXING_TASK = \"retrieval.passage\"\n",
    "QUERYING_TASK = \"retrieval.query\"\n",
    "MAX_TOKENS = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7304e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 749 entries.\n"
     ]
    }
   ],
   "source": [
    "# init qdrant\n",
    "qd_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# read json file\n",
    "with open('../dist/lotr_characters.json', 'r') as file:\n",
    "    characters = json.load(file)\n",
    "\n",
    "print(f\"Loaded {len(characters)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce78c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token(text: str)-> int:\n",
    "    return len(tokenizer.encode(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4d4c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jina_embedding(input_text: str, task = INDEXING_TASK)-> list:\n",
    "    \"\"\"\n",
    "    Create embedding using Jina API\n",
    "    Returns a single embedding vector (list of floats)\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n",
    "    }\n",
    "    data = {\n",
    "        \"input\": [input_text],\n",
    "        \"model\": JINA_EMBEDDING_MODEL,\n",
    "        \"dimensions\": EMBEDDING_DIMENSION,\n",
    "        \"task\": task,\n",
    "        \"late_chunking\": True,\n",
    "    }\n",
    "    try:\n",
    "        res = requests.post(url=JINA_URL, headers=headers, json=data, timeout=30)\n",
    "        if res.status_code == 200:\n",
    "            embedding = res.json()[\"data\"][0][\"embedding\"]\n",
    "            return embedding\n",
    "        else:\n",
    "            raise Exception(f\"Jina API error: {res.status_code} - {res.text}\")\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Request failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1d61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text_smart(text: str, max_tokens: int = 8000)-> str:\n",
    "    \"\"\"\n",
    "    Truncate text at full sentence boundaries without exceeding max_tokens.\n",
    "    Keeps as many full sentences as possible.\n",
    "    \"\"\"\n",
    "    if count_token(text=text) <= max_tokens:\n",
    "        return text\n",
    "    \n",
    "    # Split into sentences (handles ., !, ? with possible whitespace)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    truncated_sentences = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_token(sentence)\n",
    "        if total_tokens + sentence_tokens <= max_tokens:\n",
    "            truncated_sentences.append(sentence)\n",
    "            total_tokens += sentence_tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return \" \".join(truncated_sentences).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "401ef309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_text_safe(character: dict, max_tokens: int = 7000)-> str:\n",
    "    \"\"\"\n",
    "    Create character text formatted for embedding,\n",
    "    truncated safely to fit within max_tokens.\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    # Always include name\n",
    "    if character.get('name'):\n",
    "        text_parts.append(f\"Name: {character['name']}\")\n",
    "\n",
    "    # add other basic info\n",
    "    basic_fields = ['race', 'gender', 'realm', 'culture', 'birth', 'death', 'spouse', 'hair', 'height']\n",
    "    text_parts.extend(\n",
    "        f\"{field.title()}: {character[field]}\"\n",
    "        for field in basic_fields\n",
    "        if character.get(field)\n",
    "    )\n",
    "\n",
    "    # Track token budget so far\n",
    "    base_text = \"\\n\".join(text_parts)\n",
    "    base_tokens = count_token(base_text)\n",
    "    remaining_tokens = max_tokens - base_tokens\n",
    "\n",
    "    # try to include biography/history if space allows\n",
    "    for field in [\"biography\", \"history\"]:\n",
    "        content = character.get(field)\n",
    "        if content and remaining_tokens > 0:\n",
    "            truncated = truncate_text_smart(content.strip(), remaining_tokens)\n",
    "            field_text = f\"{field.title()}: {truncated}\"\n",
    "            tokens_used = count_token(field_text)\n",
    "\n",
    "            if tokens_used <= remaining_tokens:\n",
    "                text_parts.append(field_text)\n",
    "                remaining_tokens -= tokens_used\n",
    "    \n",
    "    final_text = \"\\n\".join(text_parts)\n",
    "\n",
    "    # final hard cap (in case token calc was optimistic)\n",
    "    if count_token(final_text) > max_tokens:\n",
    "        final_text = truncate_text_smart(final_text, max_tokens=max_tokens)\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a35e0eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_summary(character: dict, max_tokens: int = 500)-> str:\n",
    "    \"\"\"\n",
    "    Create a concise summary for characters with very long descriptions\n",
    "    \"\"\"\n",
    "    name = character.get('name', 'Unknown')\n",
    "    summary_parts = [name]\n",
    "\n",
    "    # add key identifiers\n",
    "    if character.get('race'):\n",
    "        summary_parts.append(f\"a {character['race']}\")\n",
    "    if character.get('realm'):\n",
    "        summary_parts.append(f\"from {character['realm']}\")    \n",
    "    if character.get('culture'):\n",
    "        summary_parts.append(f\"of {character['culture']} culture\")\n",
    "    \n",
    "    # extract first few sentences from biography/history\n",
    "    content: str = character.get('biography') or character.get('history')\n",
    "    if content is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_tokens:\n",
    "            tokens = tokens[:max_tokens]\n",
    "        first_sentences = tokenizer.decode(tokens=tokens) + \"...\"\n",
    "        summary_parts.append(first_sentences)\n",
    "    \n",
    "    return \" - \".join(summary_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fd83c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jina_embedding_batch_safe(input_texts: list, max_token_per_text: int = 6000) -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings for multiple texts with length safety checks\n",
    "    \"\"\"\n",
    "    # First, ensure all texts are within safe limits\n",
    "    safe_texts = []\n",
    "    for text in input_texts:\n",
    "        safe_text = truncate_text_smart(text=text, max_tokens=max_token_per_text)\n",
    "        safe_texts.append(safe_text)\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {JINA_API_KEY}\",\n",
    "    }\n",
    "    data = {\n",
    "        \"input\": safe_texts,\n",
    "        \"model\": JINA_EMBEDDING_MODEL,\n",
    "        \"dimensions\": EMBEDDING_DIMENSION,\n",
    "        \"task\": INDEXING_TASK,\n",
    "        \"late_chunking\": True,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        res = requests.post(url=JINA_URL, headers=headers, json=data, timeout=120)\n",
    "        if res.status_code == 200:\n",
    "            embeddings = [d[\"embedding\"] for d in res.json()[\"data\"]]\n",
    "            return embeddings\n",
    "        else:\n",
    "            raise Exception(f\"Jina API error: {res.status_code} - {res.text}\")\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Request failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fab3b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitiate_collection():\n",
    "    is_collection_exist = qd_client.collection_exists(collection_name=COLLECTION_NAME)\n",
    "    if is_collection_exist:\n",
    "        qd_client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "        print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
    "    print(f\"Collection {COLLECTION_NAME} didn't exist, creating new one\")\n",
    "    qd_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=EMBEDDING_DIMENSION, # Dimensionality of the vectors\n",
    "            distance=models.Distance.COSINE # Distance metric for similarity search\n",
    "        )\n",
    "    )\n",
    "    print(\"Created the new collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0ca041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "strategy: adaptive batching:\n",
    "- Add texts to the current batch until you're near the token budget.\n",
    "- If adding the next text would exceed the budget, start a new batch.\n",
    "- For very large texts that nearly hit the limit on their own → process them individually.\n",
    "\"\"\"\n",
    "\n",
    "def upsert_to_qdrant_adaptive(max_tokens_per_batch: int = 7000, max_tokens_per_text: int = 6000):\n",
    "    \"\"\"\n",
    "    Upsert to Qdrant with dynamic batch sizing based on token usage.\n",
    "    Each batch is sized to stay under max_tokens_per_batch.\n",
    "    Individual texts that nearly hit the limit are embedded one by one.\n",
    "    \"\"\"\n",
    "\n",
    "    if not qd_client.collection_exists(collection_name=COLLECTION_NAME):\n",
    "        print(f'Collection {COLLECTION_NAME} does not exist.')\n",
    "        return\n",
    "    # prepare safe character texts\n",
    "    print(\"Preparing safe character texts...\")\n",
    "    prepared_data = []\n",
    "    for character in characters:\n",
    "        try:\n",
    "            text = create_character_text_safe(character=character, max_tokens=max_tokens_per_text)\n",
    "            token_count = count_token(text)\n",
    "            prepared_data.append({\n",
    "                \"character\": character,\n",
    "                \"text\": text,\n",
    "                \"token_count\": token_count\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing text for {character.get('name', 'Unknown')}: {str(e)}\")\n",
    "    \n",
    "    if not prepared_data:\n",
    "        print(\"No valid character data to process\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Prepared {len(prepared_data)} characters for processing\")\n",
    "\n",
    "    # Sort shortest first → more likely to fill batches efficiently\n",
    "    prepared_data.sort(key=lambda x: x['token_count'])\n",
    "\n",
    "    # build dynamic batches\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for entry in prepared_data:\n",
    "        tc = entry['token_count']\n",
    "\n",
    "        # if one text alone is too large, process separately as its own batch\n",
    "        if tc > max_tokens_per_batch:\n",
    "            if current_batch:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = []\n",
    "                current_tokens = 0\n",
    "            batches.append([entry])\n",
    "            continue\n",
    "\n",
    "        # if adding this entry would exceed batch limit, start a new batch\n",
    "        if current_tokens + tc > max_tokens_per_batch and current_batch:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = []\n",
    "            current_tokens = 0\n",
    "\n",
    "        current_batch.append(entry)\n",
    "        current_tokens += tc\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    print(f\"built {len(batches)} batches for processing\")\n",
    "    \n",
    "    # process batches and collect points\n",
    "    all_points = []\n",
    "    total_processed = 0\n",
    "\n",
    "    for batch_num, batch in enumerate(batches, start=1):\n",
    "        texts = [e['text'] for e in batch]\n",
    "\n",
    "        print(f\"processing batch {batch_num}/{len(batches)} with {len(batch)} entries, total tokens ≈ {sum(e['token_count'] for e in batch)}\")\n",
    "\n",
    "        try:\n",
    "            embeddings = create_jina_embedding_batch_safe(texts, max_token_per_text=max_tokens_per_text)\n",
    "        except Exception as batch_error:\n",
    "            print(f\"batch {batch_num} failed: {str(batch_error)}\")\n",
    "            # fallback: process individually\n",
    "            embeddings = []\n",
    "            \n",
    "            for entry in batch:\n",
    "                try:\n",
    "                    emb = create_jina_embedding(entry['text'])\n",
    "                    embeddings.append(emb)\n",
    "                except Exception as e:\n",
    "                    print(f\"failed individual embedding for {entry['character'].get('name', 'Unknown')}: {str(e)}\")\n",
    "                    embeddings.append(None)\n",
    "                \n",
    "        # convert to qdrant points\n",
    "        for entry, embedding in zip(batch, embeddings):\n",
    "            if embedding is not None:\n",
    "                point = models.PointStruct(\n",
    "                    id=uuid.uuid4().hex,\n",
    "                    vector=embedding,\n",
    "                    payload={\n",
    "                        **entry[\"character\"],\n",
    "                        \"embedded_text\": entry[\"text\"],\n",
    "                        \"token_count\": entry[\"token_count\"]\n",
    "                    }\n",
    "                )\n",
    "                all_points.append(point)\n",
    "                total_processed += 1\n",
    "    # final upsert in one shot\n",
    "    if all_points:\n",
    "        try:\n",
    "            qd_client.upsert(collection_name=COLLECTION_NAME, points=all_points)\n",
    "            print(f\"successfully upserted {total_processed}/{len(prepared_data)} entries to qdrant\")\n",
    "        except Exception as e:\n",
    "            print(f\"final upsert failed: {str(e)}\")\n",
    "    else:\n",
    "        print(\"no valid embeddings to upsert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5c8f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, limit: int = 1):\n",
    "    \"\"\"\n",
    "    Updated search function to use Jina API for query embedding\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create embedding for the search query using Jina API\n",
    "        query_embedding = create_jina_embedding(input_text=query, task=QUERYING_TASK)\n",
    "        \n",
    "        query_points = qd_client.query_points(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            with_payload=True\n",
    "        )\n",
    "        results = [point.payload for point in query_points.points]\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc730531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_score_threshold(query: str, limit: int = 5, score_threshold: float = 0.7):\n",
    "    \"\"\"\n",
    "    Enhanced search function with similarity score filtering\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = create_jina_embedding(input_text=query, task=QUERYING_TASK)\n",
    "\n",
    "        query_points = qd_client.query_points(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            with_payload=True,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        results = [point.payload for point in query_points.points]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search with threshold: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b93cdc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: lotr-characters\n",
      "Collection lotr-characters didn't exist, creating new one\n",
      "Created the new collection\n"
     ]
    }
   ],
   "source": [
    "reinitiate_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f86e3d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing safe character texts...\n",
      "Prepared 749 characters for processing\n",
      "built 44 batches for processing\n",
      "processing batch 1/44 with 128 entries, total tokens ≈ 6953\n",
      "processing batch 2/44 with 67 entries, total tokens ≈ 6902\n",
      "processing batch 3/44 with 57 entries, total tokens ≈ 6953\n",
      "processing batch 4/44 with 50 entries, total tokens ≈ 6940\n",
      "processing batch 5/44 with 45 entries, total tokens ≈ 6868\n",
      "processing batch 6/44 with 40 entries, total tokens ≈ 6908\n",
      "processing batch 7/44 with 36 entries, total tokens ≈ 6911\n",
      "processing batch 8/44 with 33 entries, total tokens ≈ 6938\n",
      "processing batch 9/44 with 30 entries, total tokens ≈ 6938\n",
      "processing batch 10/44 with 27 entries, total tokens ≈ 6982\n",
      "processing batch 11/44 with 24 entries, total tokens ≈ 6875\n",
      "processing batch 12/44 with 22 entries, total tokens ≈ 6888\n",
      "processing batch 13/44 with 20 entries, total tokens ≈ 6751\n",
      "processing batch 14/44 with 18 entries, total tokens ≈ 6710\n",
      "processing batch 15/44 with 17 entries, total tokens ≈ 6959\n",
      "processing batch 16/44 with 15 entries, total tokens ≈ 6828\n",
      "processing batch 17/44 with 13 entries, total tokens ≈ 6564\n",
      "processing batch 18/44 with 12 entries, total tokens ≈ 6784\n",
      "processing batch 19/44 with 11 entries, total tokens ≈ 6685\n",
      "processing batch 20/44 with 10 entries, total tokens ≈ 6767\n",
      "processing batch 21/44 with 8 entries, total tokens ≈ 6256\n",
      "processing batch 22/44 with 7 entries, total tokens ≈ 6271\n",
      "processing batch 23/44 with 6 entries, total tokens ≈ 6460\n",
      "processing batch 24/44 with 6 entries, total tokens ≈ 6902\n",
      "processing batch 25/44 with 5 entries, total tokens ≈ 6053\n",
      "processing batch 26/44 with 5 entries, total tokens ≈ 6529\n",
      "processing batch 27/44 with 5 entries, total tokens ≈ 6909\n",
      "processing batch 28/44 with 4 entries, total tokens ≈ 6029\n",
      "processing batch 29/44 with 4 entries, total tokens ≈ 6260\n",
      "processing batch 30/44 with 4 entries, total tokens ≈ 6794\n",
      "processing batch 31/44 with 3 entries, total tokens ≈ 5668\n",
      "processing batch 32/44 with 2 entries, total tokens ≈ 4661\n",
      "processing batch 33/44 with 2 entries, total tokens ≈ 5417\n",
      "processing batch 34/44 with 2 entries, total tokens ≈ 5744\n",
      "processing batch 35/44 with 2 entries, total tokens ≈ 6428\n",
      "processing batch 36/44 with 1 entries, total tokens ≈ 3422\n",
      "processing batch 37/44 with 1 entries, total tokens ≈ 5100\n",
      "processing batch 38/44 with 1 entries, total tokens ≈ 5184\n",
      "processing batch 39/44 with 1 entries, total tokens ≈ 5307\n",
      "processing batch 40/44 with 1 entries, total tokens ≈ 5317\n",
      "processing batch 41/44 with 1 entries, total tokens ≈ 5565\n",
      "processing batch 42/44 with 1 entries, total tokens ≈ 5780\n",
      "processing batch 43/44 with 1 entries, total tokens ≈ 5957\n",
      "processing batch 44/44 with 1 entries, total tokens ≈ 5969\n",
      "successfully upserted 749/749 entries to qdrant\n"
     ]
    }
   ],
   "source": [
    "upsert_to_qdrant_adaptive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f7a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
