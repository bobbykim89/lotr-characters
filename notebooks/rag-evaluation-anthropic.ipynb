{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "972d89ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from tqdm import tqdm\n",
    "from os import environ\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "from composables.files import open_json_file, save_json_file\n",
    "from composables.search import llm, format_hits_response\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87e72159",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "OPENAI_TEMPERATURE = 0.5\n",
    "ANTHROPIC_API_KEY = environ.get(\"ANTHROPIC_API_KEY\")\n",
    "ANTHROPIC_MODEL = \"claude-3-5-haiku-20241022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36527ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c3d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rag_prompt (query: str, search_results: list[dict[str,str]]):\n",
    "    raw_user_prompt = \"\"\"\n",
    "Context from database:\n",
    "{retrieved_context}\n",
    "\n",
    "User question:\n",
    "{user_question}\n",
    "\n",
    "Answer the question using ONLY the context above.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "You are a helpful lore expert on J.R.R. Tolkien's Middle-earth. \n",
    "You can only answer questions about characters using the provided context retrieved from the database. \n",
    "The context includes structured information such as: name, race, titles, realm, family relations, birth and death dates, and short descriptions.\n",
    "\n",
    "Guidelines:\n",
    "- If the answer is found in the context, respond clearly and directly.\n",
    "- If the answer is not in the context, say you don’t know or that the information was not provided.\n",
    "- Do not invent new facts outside the context.\n",
    "- Keep your answers concise, but include all relevant details from the context.\n",
    "- If the user asks for speculation (e.g., \"what would happen if X met Y?\"), you can summarize based only on what the context says about their traits.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    user_prompt = raw_user_prompt.format(retrieved_context=search_results, user_question=query).strip()\n",
    "    return user_prompt, system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9579ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_eval_prompt (payload: dict[str,str])-> tuple[str, str]:\n",
    "    raw_user_prompt = \"\"\"\n",
    "Evaluate the following RAG output:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer: {answer}\n",
    "\"\"\".strip()\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "You are an impartial evaluator assessing RAG (Retrieval-Augmented Generation) system outputs for questions about J.R.R. Tolkien's Middle-earth characters.\n",
    "\n",
    "Evaluate each answer on four criteria using a 0-3 scale:\n",
    "\n",
    "1. Relevance (0-3): Does the answer directly address the question?\n",
    "   - 3: Fully addresses the question\n",
    "   - 2: Mostly relevant with minor tangents\n",
    "   - 1: Partially relevant, significant gaps\n",
    "   - 0: Irrelevant or off-topic\n",
    "\n",
    "2. Groundedness (0-3): Are all facts supported by the context?\n",
    "   - 3: All claims supported, no hallucinations\n",
    "   - 2: Mostly grounded, one minor unsupported detail\n",
    "   - 1: Multiple unsupported claims\n",
    "   - 0: Significant hallucinations or contradicts context\n",
    "\n",
    "3. Completeness (0-3): Does the answer include key details from context?\n",
    "   - 3: All important information included\n",
    "   - 2: Most key details present, minor omissions\n",
    "   - 1: Missing significant information\n",
    "   - 0: Incomplete or vague\n",
    "\n",
    "4. Faithfulness (0-3): Is the answer concise, factual, and honest about limitations?\n",
    "   - 3: Concise, factual, admits gaps appropriately\n",
    "   - 2: Mostly faithful, slightly verbose or assumes minor details\n",
    "   - 1: Invents information or doesn't admit uncertainty\n",
    "   - 0: Violates multiple guidelines\n",
    "\n",
    "Return only a JSON object with this exact structure:\n",
    "{\n",
    "  \"relevance\": <integer 0-3>,\n",
    "  \"groundedness\": <integer 0-3>,\n",
    "  \"completeness\": <integer 0-3>,\n",
    "  \"faithfulness\": <integer 0-3>,\n",
    "  \"comments\": \"<brief reasoning in 1-2 sentences>\"\n",
    "}\n",
    "\n",
    "No markdown formatting, no additional text, just the JSON object.\n",
    "\"\"\".strip()\n",
    "    user_prompt = raw_user_prompt.format(question=payload.get('question'), context=payload.get('context'), answer=payload.get('answer')).strip()\n",
    "    \n",
    "    return user_prompt, system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d033035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_anthropic(user_prompt: str, system_prompt: str):\n",
    "    message = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        system=system_prompt,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45dbcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_search_result (data: dict, query: str):\n",
    "    search_result = data.get('search_results')\n",
    "    formatted_search_result = format_hits_response(hits=search_result)\n",
    "    user_prompt, system_prompt = format_rag_prompt(query=query, search_results=formatted_search_result)\n",
    "    res = llm(user_prompt=user_prompt, system_prompt=system_prompt)\n",
    "    return res, formatted_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5532c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_eval_with_retrieval_results_anthropic(data: dict):\n",
    "    search_result = data.get('search_results')\n",
    "    question = data.get('question')\n",
    "    formatted_search_result = format_hits_response(hits=search_result)\n",
    "    rag_user_prompt, rag_sys_prompt = format_rag_prompt(query=question, search_results=formatted_search_result)\n",
    "    answer = llm(user_prompt=rag_user_prompt, system_prompt=rag_sys_prompt)\n",
    "    payload = {\n",
    "        \"question\": question,\n",
    "        \"context\": search_result,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    eval_user_prompt, eval_sys_prompt = format_eval_prompt(payload=payload)\n",
    "    res = llm_anthropic(user_prompt=eval_user_prompt, system_prompt=eval_sys_prompt)\n",
    "\n",
    "    if type(res) == str:\n",
    "        json_res = json.loads(res)\n",
    "        return {\"question\": question, \"answer\": answer, **json_res}\n",
    "    else:\n",
    "        return {\"question\": question, \"answer\": answer, **res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6319d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_eval_result_with_retrieval_results(data: list[dict]):\n",
    "    eval_results = []\n",
    "    for retrieval_result in tqdm(data, desc=\"Processing documents\"):\n",
    "        result = rag_eval_with_retrieval_results_anthropic(data=retrieval_result)\n",
    "        eval_results.append(result)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb03b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_evaluation_result_anthropic (file_path: str):\n",
    "    eval_data: list[dict] = open_json_file(file_path=file_path)\n",
    "    num_entries = len(eval_data)\n",
    "    \n",
    "    relevance_total = 0\n",
    "    groundedness_total = 0\n",
    "    completeness_total = 0\n",
    "    faithfulness_total = 0\n",
    "\n",
    "    for entry in tqdm(eval_data, desc=\"Processing data\"):\n",
    "        relevance = entry.get(\"relevance\")\n",
    "        groundedness = entry.get(\"groundedness\")\n",
    "        completeness = entry.get(\"completeness\")\n",
    "        faithfulness = entry.get(\"faithfulness\")\n",
    "\n",
    "        relevance_total += relevance\n",
    "        groundedness_total += groundedness\n",
    "        completeness_total += completeness\n",
    "        faithfulness_total += faithfulness\n",
    "    \n",
    "    avg_relevance = relevance_total / num_entries\n",
    "    avg_groundedness = groundedness_total / num_entries\n",
    "    avg_completeness = completeness_total / num_entries\n",
    "    avg_faithfulness = faithfulness_total / num_entries\n",
    "    total_avg_score = (relevance_total + groundedness_total + completeness_total + faithfulness_total) / (num_entries * 4)\n",
    "\n",
    "    print(\"\"\"\n",
    "Evaluation using claude-3-5-haiku-20241022\n",
    "Evaluate on four criteria:\n",
    "1. Relevance (0-3): Does the answer directly address the question?\n",
    "   - 3: Fully addresses the question\n",
    "   - 2: Mostly relevant with minor tangents\n",
    "   - 1: Partially relevant, significant gaps\n",
    "   - 0: Irrelevant or off-topic\n",
    "\n",
    "2. Groundedness (0-3): Are all facts supported by the context?\n",
    "   - 3: All claims supported, no hallucinations\n",
    "   - 2: Mostly grounded, one minor unsupported detail\n",
    "   - 1: Multiple unsupported claims\n",
    "   - 0: Significant hallucinations or contradicts context\n",
    "\n",
    "3. Completeness (0-3): Does the answer include key details from context?\n",
    "   - 3: All important information included\n",
    "   - 2: Most key details present, minor omissions\n",
    "   - 1: Missing significant information\n",
    "   - 0: Incomplete or vague\n",
    "\n",
    "4. Faithfulness (0-3): Is the answer concise, factual, and honest about limitations?\n",
    "   - 3: Concise, factual, admits gaps appropriately\n",
    "   - 2: Mostly faithful, slightly verbose or assumes minor details\n",
    "   - 1: Invents information or doesn't admit uncertainty\n",
    "   - 0: Violates multiple guidelines\n",
    "\"\"\")\n",
    "    print(f\"Number of entries: {num_entries}\")\n",
    "    print(f\"Average Relevance Score: {avg_relevance}\")\n",
    "    print(f\"Average Groundedness Score: {avg_groundedness}\")\n",
    "    print(f\"Average Completeness Score: {avg_completeness}\")\n",
    "    print(f\"Average Faithfulness Score: {avg_faithfulness}\")\n",
    "    print(f\"Total Average Score: {total_avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dbea43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_search_results = open_json_file(file_path=\"../dist/retrieval_search_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0e06509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 500/500 [1:00:35<00:00,  7.27s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results_anthropic = generate_rag_eval_result_with_retrieval_results(data=raw_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de19d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_file(data=eval_results_anthropic, file_path=\"../dist/evaluation_results_claude_3_5_haiku.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6debfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 500/500 [00:00<00:00, 2135592.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation using claude-3-5-haiku-20241022\n",
      "Evaluate on four criteria:\n",
      "1. Relevance (0-3): Does the answer directly address the question?\n",
      "   - 3: Fully addresses the question\n",
      "   - 2: Mostly relevant with minor tangents\n",
      "   - 1: Partially relevant, significant gaps\n",
      "   - 0: Irrelevant or off-topic\n",
      "\n",
      "2. Groundedness (0-3): Are all facts supported by the context?\n",
      "   - 3: All claims supported, no hallucinations\n",
      "   - 2: Mostly grounded, one minor unsupported detail\n",
      "   - 1: Multiple unsupported claims\n",
      "   - 0: Significant hallucinations or contradicts context\n",
      "\n",
      "3. Completeness (0-3): Does the answer include key details from context?\n",
      "   - 3: All important information included\n",
      "   - 2: Most key details present, minor omissions\n",
      "   - 1: Missing significant information\n",
      "   - 0: Incomplete or vague\n",
      "\n",
      "4. Faithfulness (0-3): Is the answer concise, factual, and honest about limitations?\n",
      "   - 3: Concise, factual, admits gaps appropriately\n",
      "   - 2: Mostly faithful, slightly verbose or assumes minor details\n",
      "   - 1: Invents information or doesn't admit uncertainty\n",
      "   - 0: Violates multiple guidelines\n",
      "\n",
      "Number of entries: 500\n",
      "Average Relevance Score: 2.938\n",
      "Average Groundedness Score: 2.976\n",
      "Average Completeness Score: 2.728\n",
      "Average Faithfulness Score: 2.976\n",
      "Total Average Score: 2.9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_evaluation_result_anthropic(file_path=\"../dist/evaluation_results_claude_3_5_haiku.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4b1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_eval_df = pd.DataFrame(data=eval_results_anthropic)\n",
    "anthropic_eval_df.to_csv(\"../dist/evaluation_results_claude_3_5_haiku.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfe5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
